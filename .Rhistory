library(DescTools)
library(Amelia)
library(corrplot)
library(caret)
library(ISLR)
library(SmartEDA)
#heart<-read.csv("D:/BITS/2020 S2/Predictive Analytics/heart_failure_clinical_records_dataset.csv",stringsAsFactors = TRUE)
data<-read.csv("C:/R/PA/heart_failure_clinical_records_dataset.csv",stringsAsFactors = TRUE)
?ExpData
?names(data)
head(data)
tail(data)
names(data)
ExpData(data=data,type=1)
ExpData(data=data,type=2)
data$sex<-as.factor(data$sex)
data$smoking<-as.factor(data$smoking)
data$DEATH_EVENT<-as.factor(data$DEATH_EVENT)
data$anaemia<-as.factor(data$anaemia)
data$diabetes<-as.factor(data$diabetes)
data$high_blood_pressure<-as.factor(data$high_blood_pressure)
# Rerun ExpData command to note the chnage in data types of the variables
# Overall data summary
ExpData(data=data,type=1)
# Variable level data summary
ExpData(data=data,type=2)
# Some descriptive Statistics
summary(data)
Desc(data)
dim(data)
ExpNumStat(data,by="A",gp=NULL,Qnt=seq(0,1,0.1),MesofShape=2,Outlier=TRUE,round=2,Nlim=10)
ExpCTable(data,Target=NULL,margin=1,clim=10,nlim=3,round=2,bin=NULL,per=T)
# distribution of class variable
y <- data$DEATH_EVENT
cbind(freq=table(y), percentage=prop.table(table(y))*100)
cat_var<-sapply(data,FUN=is.factor)
cat_var1<-which(cat_var)
cat_var2<-which(!cat_var)
par(mfrow=c(1,3))
for(i in which(!cat_var)) {
hist(data[,i],breaks =30, main=names(data)[i])
boxplot(data[,i],main=names(data)[i])
plot(density(data[,i]))
}
for(i in which(cat_var)) {
counts<-table(data[,i])
name<-names(data)[i]
barplot(counts, main=name)
}
# create a missing map
par(mfrow = c(1,1))
missmap(data, col=c("black", "grey"), legend=FALSE)
correlations <- round(cor(data[,!cat_var]),3)
# create correlation plot
corrplot(correlations, method="circle")
pairs(data[,!cat_var])
pairs(DEATH_EVENT~.,data = data[,c(12,cat_var2)],col= data$DEATH_EVENT)
prop.table(table(data$DEATH_EVENT,data$smoking),margin=1)
prop.table(table(data$DEATH_EVENT,data$smoking),margin=2)
chisq.test(data$DEATH_EVENT,data$smoking)
chisq.test(data$DEATH_EVENT,data$diabetes)
chisq.test(data$DEATH_EVENT,data$high_blood_pressure)
# density plots for each attribute by class value
x <- data[,cat_var2]
y <- data[,12]
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)
featurePlot(x=x, y=y, plot="box",scales=scales)
#IMputation
set.seed(100)
v1<-rnorm(100,5,2)
v2<-rgamma(100, shape = 3, rate = 0.2)
random_blanks<-sample(1:100,5)
v2[random_blanks]<-NA
v1
order(v1)
v2[21]<-qgamma(0.33,shape = 3, rate = .2)
v2
v2[21]<-qgamma(runif(1),shape = 3, rate = .2)
v2
v2[21]
v2[21]<-qgamma(runif(1),shape = 3, rate = .2)
v2[21]
library(caret)
# summarize data
summary(data)
# calculate the pre-process parameters from the dataset
preprocessParams <- preProcess(data[,c(1,3,5,7,8,9)], method=c("scale"))
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, data[,c(1,3,5,7,8,9)])
# summarize the transformed dataset
summary(transformed)
preprocessParams <- preProcess(data[,c(1,3,5,7,8,9)], method=c("center"))
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, data[,c(1,3,5,7,8,9)])
preprocessParams <- preProcess(data[,c(1,3,5,7,8,9)], method=c("center", "scale"))
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, data[,c(1,3,5,7,8,9)])
# summarize the transformed dataset
summary(transformed)
preprocessParams <- preProcess(data[,c(1,3,5,7,8,9)], method=c("range"))
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, data[,c(1,3,5,7,8,9)])
# summarize the transformed dataset
summary(transformed)
basicStats(data[,c(1,3,5,7,8,9)])
install.packages("fBasics")
#install.packages("fBasics")
library(fBasics)
basicStats(data[,c(1,3,5,7,8,9)])
preprocessParams <- preProcess(data[,c(1,3,5,7,8,9)], method=c("BoxCox"))
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, data[,c(1,3,5,7,8,9)])
# summarize the transformed dataset
summary(transformed)
basicStats(transformed)
preprocessParams <- preProcess(data[,c(1,3,5,7,8,9)], method=c("YeoJohnson"))
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, data[,c(1,3,5,7,8,9)])
# summarize the transformed dataset
summary(transformed)
basicStats(transformed)
preprocessParams <- preProcess(data[,c(1,3,5,7,8,9)], method=c("center", "scale", "pca"))
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, data[,c(1,3,5,7,8,9)])
# summarize the transformed dataset
summary(transformed)
prcomp(data[,c(1,3,5,7,8,9)])
options(scipen=7)
prcomp(data[,c(1,3,5,7,8,9)],scale.=TRUE)
cor(transformed)
# summarize transform parameters
print(preprocessParams)
preprocessParams <- preProcess(data[,c(1,3,5,7,8,9)], method=c("center", "scale", "ica","YeoJohnson"), n.comp = 4)
install.packages("fastICA")
preprocessParams <- preProcess(data[,c(1,3,5,7,8,9)], method=c("center", "scale", "ica","YeoJohnson"), n.comp = 4)
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, data[,c(1,3,5,7,8,9)])
# summarize the transformed dataset
summary(transformed)
Desc(transformed)
# Injecting missing value
set.seed(100)
data[sample(1:nrow(data), 5), "age"] <- NA
data[sample(1:nrow(data), 10), "CP"] <- NA
data[sample(1:nrow(data), 8), "serum_creatinine"] <- NA
summary(data)
#*****************************************************
#identify missing values
any(is.na(data))
# Using complete.cases() function to get percentage of missing value
nrow(data[!complete.cases(data), ])/nrow(data)*100
# Using complete.cases() function to get percentage of missing value
nrow(data[!complete.cases(data), ])/nrow(data)*100
library(mice)
install.packages("mice")
library(mice)
data1 <- data[complete.cases(data), ]
# or we can use na.omit() function
data1 <- na.omit(data)
#DELETE VARIABLES WITH MISSING VALUES
## Removing columns with more than 30% NA
data[, -which(colMeans(is.na(data)) > 0.3)]
md.pattern(data)
#*****************************************************
#identify missing values
any(is.na(data))
# Using complete.cases() function to get percentage of missing value
nrow(data[!complete.cases(data), ])/nrow(data)*100
md.pattern(data)
data1 <- data[complete.cases(data), ]
# or we can use na.omit() function
data1 <- na.omit(data)
#DELETE VARIABLES WITH MISSING VALUES
## Removing columns with more than 30% NA
data[, -which(colMeans(is.na(data)) > 0.3)]
colMeans(is.na(data)
colMeans(is.na(data))
colMeans(is.na(data))
md.pattern(data)
library(Hmisc)
data$age<-impute(data$age, mean)  # replace with mean
data$CP<-impute(data$CP, median)  # median
#************************************************************
#MULTIVARIATE IMPUTATION BY CHAINED EQUATIONS
#missing values are  replaced by Predictive Mean Matching (PMM) or Logistic Regression or Proportional Odds Model
imputed_data <- mice(data, m=5, method = 'pmm', seed = 100)
# checking the summary
summary(imputed_data)
# Checking imputed values
imputed_data$imp
#USING MACHINE LEARNING ALGORITHMS
#USING KNN TO FILL THE MISSING VALUES
library(bnstruct)
install.packages(bnstruct)
install.packages('bnstruct')
library(bnstruct)
imputed_data <- mice(data, m=5, method = 'pmm', seed = 100)
# checking the summary
summary(imputed_data)
data2<-knn.impute(as.matrix(data), k = 10, cat.var = c(2,4,6,10,11,12), to.impute = 1:nrow(data),
using = 1:nrow(data))
summary(data2)
# Imputing values using Random forest
library(randomForest)
set.seed(100)
data3 <- rfImpute(DEATH_EVENT ~ ., data)
summary(data3)
# Imputing values using Random forest
library(randomForest)
#USING MACHINE LEARNING ALGORITHMS
#USING KNN TO FILL THE MISSING VALUES
library(bnstruct)
data2<-knn.impute(as.matrix(data), k = 10, cat.var = c(2,4,6,10,11,12), to.impute = 1:nrow(data),
using = 1:nrow(data))
data2<-knn.impute(as.matrix(data), k = 10, cat.var = c(2,4,6,10,11,12), to.impute = 1:nrow(data),using = 1:nrow(data))
library(bnstruct)
imputed_data <- mice(data, m=5, method = 'pmm', seed = 100)
library(DescTools)
library(corrplot)
library(caret)
library(ISLR)
library(SmartEDA)
data<-read.csv("C:/R/PA/BAZG512_EC2M_Data.csv",stringsAsFactors = TRUE)
names(data)
ExpData(data=data,type=1)  # Overall data summary
ExpData(data=data,type=2)  # Variable level data summary
# Some descriptive Statistics
summary(data)
Desc(data)
dim(data)
#* **********************************************************************************
#* Q2. Impute the missing values of Meta Score and Gross columns with the first quartile value
#*     Compute the mean meta score and mean Gross for movies with IMDB Rating of below 8
#*
#*
#*
library(mice)
md.pattern(data)
library(Hmisc)
ExpNumStat(data,by="A",gp=NULL,Qnt=seq(0,1,0.1),MesofShape=2,Outlier=TRUE,round=2,Nlim=10)
ExpCTable(data,Target=NULL,margin=1,clim=10,nlim=3,round=2,bin=NULL,per=T)
# distribution of class variable
y <- data$IMDB_Above8
cbind(freq=table(y), percentage=prop.table(table(y))*100)
#Single variable Visualizations
# create histograms for each attribute
cat_var<-sapply(data,FUN=is.factor)
cat_var1<-which(cat_var)
cat_var2<-which(!cat_var)
par(mfrow=c(1,3))
for(i in which(cat_var)) {
counts<-table(data[,i])
name<-names(data)[i]
barplot(counts, main=name)
}
correlations <- round(cor(data[,!cat_var]),3)
# Create Correlation plot
corrplot(correlations, method="circle")
pairs(data[,!cat_var])
pairs(IMDB_Above8~.,data = data[,c(24,cat_var2)],col= data$IMDB_Above8)
#* **********************************************************************************
#* Q2. Impute the missing values of Meta Score and Gross columns with the first quartile value
#*     Compute the mean meta score and mean Gross for movies with IMDB Rating of below 8
#*
#*
#*
library(mice)
# distribution of class variable
y <- data$IMDB_Above8
cbind(freq=table(y), percentage=prop.table(table(y))*100)
#Single variable Visualizations
# create histograms for each attribute
cat_var<-sapply(data,FUN=is.factor)
cat_var1<-which(cat_var)
cat_var2<-which(!cat_var)
par(mfrow=c(1,3))
for(i in which(cat_var)) {
counts<-table(data[,i])
name<-names(data)[i]
barplot(counts, main=name)
}
correlations <- round(cor(data[,!cat_var]),3)
# Create Correlation plot
corrplot(correlations, method="circle")
pairs(data[,!cat_var])
#* **********************************************************************************
#* Q2. Impute the missing values of Meta Score and Gross columns with the first quartile value
#*     Compute the mean meta score and mean Gross for movies with IMDB Rating of below 8
#*
#*
#*
library(mice)
md.pattern(data)
library(Hmisc)
data$Meta_score<-impute(data$Meta_score, summary(data$Meta_score)[2] )  # replace with quartile 1
data$Gorss<-impute(data$Gross, summary(data$Gross)[2] )  # replace with quartile 1
md.pattern(data)
data$Meta_score<-impute(data$Meta_score, summary(data$Meta_score)[2] )  # replace with quartile 1
data$Gross<-impute(data$Gross, summary(data$Gross)[2] )  # replace with quartile 1
md.pattern(data)
summary(data)
data2 <- data[which(data$IMDB_Above8 == "Yes"),]
data3 <- data[which(data$IMDB_Above8 == "No"),]
mean(data2$Meta_score)
mean(data2$Gross)
mean(data3$Meta_score)
mean(data3$Gross)
library(Amelia)
library(imputeTS)
library(fBasics)
data<-read.csv("C:/R/PA/BAZG512_EC2M_Data.csv",stringsAsFactors = TRUE)
data2 <-  data[!(is.na(data$Certificate) | data$Certificate==""), ]
data3 <- data[data$Comedy == "1"| data$Animation == "1" | data$Mystery == "1",]
set.seed(100)  # For reproducibility
# Create index for testing and training data
inTrain <- createDataPartition(y = data3$IMDB_Above8, p = 0.7, list = FALSE)
# subset titanic data to training
training <- data3[inTrain,]
# subset the rest to test
testing <- data3[-inTrain,]
finalset <- testing[which(testing$IMDB_Above8 == "Yes"),]
prop.table(finalset[,5:20])
#* Create new dataset with movies belonging to any one or more of Crime or action or Biographic categories
#* If a movie belongs to one or more categories along with any of the other categories then also, the movie needs to
#* be included in the subset
data3 <- data[data$Comedy == "1"| data$Mystery == "1",]
set.seed(100)  # For reproducibility
# Create index for testing and training data
inTrain <- createDataPartition(y = data3$IMDB_Above8, p = 0.7, list = FALSE)
# subset titanic data to training
training <- data3[inTrain,]
# subset the rest to test
testing <- data3[-inTrain,]
finalset <- testing[which(testing$IMDB_Above8 == "Yes"),]
prop.table(finalset[,5:20])
finalset <- testing[which(testing$IMDB_Above8 == "Yes"),]
prop.table(finalset[,5:10])
data<-read.csv("C:/R/PA/BAZG512_EC2M_Data.csv",stringsAsFactors = TRUE)
summary(data)
data$Meta_score<-impute(data$Meta_score, summary(data$Meta_score)[2] )  # replace with quartile 1
data$Gross<-impute(data$Gross, summary(data$Gross)[2] )  # replace with quartile 1
md.pattern(data)
summary(data)
# calculate the pre-process parameters from the dataset on numeric columns
preprocessParams <- preProcess(data[,c(1,2,3,4,5,7,8,9,10,11,12,13,14,15,16,17,17,18,19,20,21,22,23,24)], method=c("scale"))
# calculate the pre-process parameters from the dataset on numeric columns
preprocess <- preProcess(data[,c(1,2,3,4,5,7,8,9,10,11,12,13,14,15,16,17,17,18,19,20,21,22,23,24)], method=c("scale"))
# summarize transform parameters
print(preprocess)
# transform the dataset using the parameters
transformed <- predict(preprocess, data[,c(1,2,3,4,5,7,8,9,10,11,12,13,14,15,16,17,17,18,19,20,21,22,23,24)])
# summarize the transformed dataset
summary(transformed)  # scale expreses each observation in terms of no. of Standard deviations
